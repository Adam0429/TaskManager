##====================================================================
## Configuration for EMQ X Kafka Bridge
##====================================================================

## Cluster support
## bridge.kafka.servers = 127.0.0.1:9092,127.0.0.2:9092,127.0.0.3:9092
bridge.kafka.servers = 127.0.0.1:9092

## When connecting to kafka 0.9 query_api_versions config entry should be set to false
## otherwise the socket will be closed by kafka.
bridge.kafka.query_api_versions = true

## default per_partition, can also be per_broker.
## This is to configure how client manages connections:
## one connection per-partition or one connection per-broker.
## per_partition may give better throughput, but it could be
## quite exhausting for both beam and kafka cluster when
## there is a great number of partitions
bridge.kafka.connection_strategy = per_partition

## This is to avoid excessive metadata refresh and partition
## leader reconnect when a lot of connections restart around the same moment.
## Also, when kafka partition leader broker is down,
## it usually takes a few seconds to get a new leader elacted,
## hence it is a good idea to have a delay before trying to reconnect.
bridge.kafka.min_metadata_refresh_interval = 5S

## Pick a partition producer and sync/async.
bridge.kafka.produce = sync

bridge.kafka.produce.sync_timeout = 3s

## Base directory for replayq to store messages on disk.
## If this config entry if missing or set to undefined,
## replayq works in a mem-only manner.
## i.e. messages are not queued on disk -- in such case,
## the send or send_sync API callers are responsible for
## possible message loss in case of application,
## network or kafka disturbances. For instance,
## in the wolff:send API caller may trap_exit then
## react on parition-producer worker pid's 'EXIT'
## message to issue a retry after restarting the producer.
## bridge.kafka.replayq_dir = data/replayq/

## default=false, use replayq in offload mode
## In 'offload' mode, segment files are not for data persistence
## but only to offload RAM.
## bridge.kafka.replayq_offload_mode = false

## default=10MB, replayq segment size.
## bridge.kafka.producer.replayq_seg_bytes = 10MB

## all_isr, leader_only or none, see kafka_protocol lib doc.
##bridge.kafka.producer.required_acks = none

## default=10000. Timeout leader wait for replicas before reply to producer.
##bridge.kafka.producer.ack_timeout = 10S

## default number of message sets sent on wire before block waiting for acks
##bridge.kafka.producer.max_batch_bytes = 1024KB

## by default, send max 1 MB of data in one batch (message set)
##bridge.kafka.producer.min_batch_bytes = 0

## Number of batches to be sent ahead without receiving ack for the last request.
## Must be 0 if messages must be delivered in strict order.
##bridge.kafka.producer.max_send_ahead = 0

## by default, no compression
##bridge.kafka.producer.compression = no_compression

## bridge.kafka.encode_payload_type = base64

## bridge.kafka.sock.buffer = 32KB
## bridge.kafka.sock.recbuf = 32KB
bridge.kafka.sock.sndbuf = 1MB
## bridge.kafka.sock.read_packets = 20

## Bridge Kafka Hooks
## ${topic}: the kafka topics to which the messages will be published.
## ${filter}: the mqtt topic (may contain wildcard) on which the action will be performed .
bridge.kafka.hook.client.connected.1     = {"topic":"ClientConnected"}
bridge.kafka.hook.client.disconnected.1  = {"topic":"ClientDisconnected"}
bridge.kafka.hook.session.subscribed.1   = {"filter":"#", "topic":"SessionSubscribed"}
bridge.kafka.hook.session.unsubscribed.1 = {"filter":"#", "topic":"SessionUnsubscribed"}
bridge.kafka.hook.message.publish.1      = {"filter":"#", "topic":"MessagePublish"}
bridge.kafka.hook.message.delivered.1    = {"filter":"#", "topic":"MessageDelivered"}
bridge.kafka.hook.message.acked.1        = {"filter":"#", "topic":"MessageAcked"}

## More Configures
## partitioner strategy:
## Option:  random | roundrobin | first_key_dispatch
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "strategy":"random"}

## key:
## Option: ${clientid} | ${username}
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "key":"${clientid}"}

## format:
## Option: json | json
## Example: bridge.kafka.hook.message.publish.1 = {"filter":"#", "topic":"MessagePublish", "format":"json"}
